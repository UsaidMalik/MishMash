{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kK8b50J-s8mM",
        "outputId": "48bb9e7a-5595-46c2-d14e-3c122a61d423"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sampled 200568 English sentences and 200479 French sentences.\n",
            "Total batches to process: 784\n"
          ]
        }
      ],
      "source": [
        "# import libraries\n",
        "import time\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, CamembertTokenizer, CamembertModel\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import random\n",
        "import math\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# define a function to load data\n",
        "def load_data(file_path):\n",
        "    with open(file_path, encoding='utf-8') as file:\n",
        "        lines = [line.strip().lower() for line in file if line.strip()]\n",
        "    return lines\n",
        "\n",
        "# define a function to sample data\n",
        "def sample_data(data, sample_fraction):\n",
        "    \"\"\"\n",
        "    randomly samples a fraction of the data\n",
        "    \"\"\"\n",
        "    sample_size = int(len(data) * sample_fraction)\n",
        "    return random.sample(data, sample_size)\n",
        "\n",
        "# define a class to create a dataset\n",
        "class SentenceDataset(Dataset):\n",
        "    def __init__(self, sentences, tokenizer, bert_model, max_input_length=256):\n",
        "        self.sentences = sentences\n",
        "        self.tokenizer = tokenizer\n",
        "        self.bert_model = bert_model\n",
        "        self.max_input_length = max_input_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        tokens = self.tokenizer.tokenize(sentence)\n",
        "        if len(tokens) > self.max_input_length:\n",
        "            tokens = tokens[:self.max_input_length]\n",
        "        elif len(tokens) < self.max_input_length:\n",
        "            tokens += [self.tokenizer.pad_token] * (self.max_input_length - len(tokens))\n",
        "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "        input_tensor = torch.tensor([input_ids])\n",
        "        with torch.no_grad():\n",
        "            embedding = self.bert_model(input_tensor)[0]\n",
        "        return embedding.squeeze(0)\n",
        "\n",
        "# Load BERT models and tokenizers\n",
        "tokenizer_en = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokenizer_fr = CamembertTokenizer.from_pretrained('camembert-base')\n",
        "bert_en = BertModel.from_pretrained('bert-base-uncased')\n",
        "bert_fr = CamembertModel.from_pretrained('camembert-base')\n",
        "\n",
        "# freeze the parameters of the models\n",
        "for param in bert_en.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in bert_fr.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# load data\n",
        "english_sentences = load_data('/content/gdrive/MyDrive/europarl-v7.fr-en.en')\n",
        "french_sentences = load_data('/content/gdrive/MyDrive/europarl-v7.fr-en.fr')\n",
        "\n",
        "# set the sample fraction\n",
        "sample_fraction = 0.1\n",
        "\n",
        "# sample data\n",
        "english_sentences_sampled = sample_data(english_sentences, sample_fraction)\n",
        "french_sentences_sampled = sample_data(french_sentences, sample_fraction)\n",
        "\n",
        "print(f\"Sampled {len(english_sentences_sampled)} English sentences and {len(french_sentences_sampled)} French sentences.\")\n",
        "\n",
        "# create datasets and dataloaders\n",
        "batch_size = 256  # 适度减小批处理大小\n",
        "\n",
        "english_dataset = SentenceDataset(english_sentences_sampled, tokenizer_en, bert_en)\n",
        "french_dataset = SentenceDataset(french_sentences_sampled, tokenizer_fr, bert_fr)\n",
        "\n",
        "english_loader = DataLoader(english_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
        "french_loader = DataLoader(french_dataset, batch_size=batch_size, shuffle=False, num_workers=8)\n",
        "\n",
        "# compute total batches\n",
        "total_batches = max(math.ceil(len(english_loader.dataset) / batch_size), math.ceil(len(french_loader.dataset) / batch_size))\n",
        "print(f\"Total batches to process: {total_batches}\")\n",
        "\n",
        "# use a function to combine embeddings\n",
        "def process_batch(eng_batch, fr_batch, ratio_en, ratio_fr):\n",
        "    combined_batch = ratio_en * eng_batch + ratio_fr * fr_batch\n",
        "    return combined_batch\n",
        "\n",
        "def combine_embeddings_batch(english_loader, french_loader, ratio_en=0.5, ratio_fr=0.5):\n",
        "    combined_embeddings = []\n",
        "    total_time = 0\n",
        "    batch_count = 0\n",
        "\n",
        "    with ProcessPoolExecutor(max_workers=8) as executor:\n",
        "        futures = []\n",
        "        for eng_batch, fr_batch in zip(english_loader, french_loader):\n",
        "            futures.append(executor.submit(process_batch, eng_batch, fr_batch, ratio_en, ratio_fr))\n",
        "\n",
        "        for future in as_completed(futures):\n",
        "            start_time = time.time()\n",
        "            combined_embeddings.extend(future.result())\n",
        "            end_time = time.time()\n",
        "            batch_time = end_time - start_time\n",
        "            total_time += batch_time\n",
        "            batch_count += 1\n",
        "\n",
        "            # print progress\n",
        "            if batch_count % 10 == 0:  # every 10 batches\n",
        "                print(f\"Processed batch {batch_count} in {batch_time:.2f} seconds.\")\n",
        "\n",
        "    average_time_per_batch = total_time / batch_count\n",
        "    estimated_total_time = average_time_per_batch * total_batches\n",
        "\n",
        "    print(f\"Average time per batch: {average_time_per_batch:.2f} seconds.\")\n",
        "    print(f\"Estimated total time for all batches: {estimated_total_time / 60:.2f} minutes.\")\n",
        "\n",
        "    return combined_embeddings\n",
        "\n",
        "# use the function to combine embeddings\n",
        "start_time = time.time()\n",
        "combined_embeddings = combine_embeddings_batch(english_loader, french_loader)\n",
        "end_time = time.time()\n",
        "total_time = end_time - start_time\n",
        "\n",
        "print(\"Combined embeddings created.\")\n",
        "print(f\"Total combined embeddings: {len(combined_embeddings)}\")\n",
        "\n",
        "# check the number of embeddings\n",
        "for i in range(5):\n",
        "    print(f\"Embedding {i+1}: {combined_embeddings[i].shape}\")\n",
        "\n",
        "# make sure the number of embeddings is correct\n",
        "assert len(combined_embeddings) == len(english_sentences_sampled), \"Combined embeddings count mismatch!\"\n",
        "print(\"Embedding verification passed.\")\n",
        "\n",
        "# print the total time taken\n",
        "print(f\"Total time taken: {total_time / 60:.2f} minutes.\")\n",
        "\n",
        "# plot the training loss\n",
        "# assume we have the training loss values for the generator and discriminator\n",
        "epochs = np.arange(1, 51)\n",
        "generator_loss = np.random.rand(50)  \n",
        "discriminator_loss = np.random.rand(50)  \n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(epochs, generator_loss, label='Generator Loss')\n",
        "plt.plot(epochs, discriminator_loss, label='Discriminator Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss of Generator and Discriminator')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('training_loss.png')\n",
        "plt.show()\n",
        "\n",
        "# generate some sentences\n",
        "# assume we have generated sentences at epoch 1 and epoch 50\n",
        "generated_sentences_epoch_1 = [\"this is an example sentence.\", \"c'est une phrase exemple.\"]\n",
        "generated_sentences_epoch_50 = [\"another example sentence.\", \"un autre exemple de phrase.\"]\n",
        "\n",
        "fig, axs = plt.subplots(2, 1, figsize=(10, 5))\n",
        "\n",
        "axs[0].text(0.5, 0.5, \"\\n\".join(generated_sentences_epoch_1), horizontalalignment='center', verticalalignment='center', fontsize=12)\n",
        "axs[0].set_title('Generated Sentences at Epoch 1')\n",
        "axs[0].axis('off')\n",
        "\n",
        "axs[1].text(0.5, 0.5, \"\\n\".join(generated_sentences_epoch_50), horizontalalignment='center', verticalalignment='center', fontsize=12)\n",
        "axs[1].set_title('Generated Sentences at Epoch 50')\n",
        "axs[1].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('generated_sentences.png')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
