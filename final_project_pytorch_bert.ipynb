{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing my libraries"
      ],
      "metadata": {
        "id": "12hlOUtu6hBp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "import tarfile\n",
        "import numpy as np\n",
        "import cupy as cp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n"
      ],
      "metadata": {
        "id": "XujZHQqE6e4L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5efb215-c433-4f77-8b6e-fb15a3b840b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# transformers and the tokenzier for transformers\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from transformers import CamembertTokenizer, CamembertModel\n",
        "import torch.nn as nn\n",
        "# the french tokenizer\n",
        "\n",
        "tokenizer_en = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "tokenizer_fr = CamembertTokenizer.from_pretrained('camembert-base')\n",
        "\n",
        "# declaring special tokens\n",
        "\n",
        "bert_en = BertModel.from_pretrained('bert-base-uncased')\n",
        "bert_fr = CamembertModel.from_pretrained('camembert-base')\n",
        "\n",
        "# Freeze the BERTs model\n",
        "for param in bert_en.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "\n",
        "# Freeze the BERT model\n",
        "for param in bert_fr.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "def tokenize_and_cut(sentence, tokenizer, max_input_length = 512):\n",
        "    tokens = tokenizer.tokenize(sentence)\n",
        "    if len(tokens) > max_input_length: # start and end toks\n",
        "        # Truncate the tokens\n",
        "        tokens = tokens[:max_input_length]\n",
        "    elif len(tokens) < max_input_length:\n",
        "        # Pad the tokens\n",
        "        tokens += [tokenizer.pad_token] * (max_input_length - len(tokens))\n",
        "    return tokens\n",
        "# the tokenizer function"
      ],
      "metadata": {
        "id": "E5rv9i9Gocvb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edea9fa4-6326-4066-ac5b-70edb9564106"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test and tokenzie\n",
        "import torch\n",
        "tokens = tokenize_and_cut(\"Comel is  are you?\", tokenizer_fr)\n",
        "print(tokens)\n",
        "input_ids = tokenizer_fr.convert_tokens_to_ids(tokens)\n",
        "print(input_ids)\n",
        "input_tensor = torch.tensor([input_ids])\n",
        "\n",
        "embedding = bert_fr(input_tensor)[0]\n",
        "print(embedding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LMS6_f6QBTbs",
        "outputId": "620cfa27-ca5b-4acf-d62d-021880c21fe7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['▁Com', 'el', '▁is', '▁are', '▁you', '?']\n",
            "[2650, 647, 2856, 9581, 4835, 197]\n",
            "tensor([[[ 0.0057,  0.1593,  0.1688,  ..., -0.0633, -0.0175,  0.0133],\n",
            "         [ 0.0319,  0.6169, -0.0538,  ..., -0.0099,  0.0261,  0.1721],\n",
            "         [ 0.0476,  0.1551, -0.0816,  ...,  0.0124, -0.0178,  0.3162],\n",
            "         [ 0.0437,  0.0374, -0.1425,  ...,  0.0432, -0.0914,  0.2645],\n",
            "         [ 0.2930,  0.0210,  0.0599,  ..., -0.0242,  0.0299,  0.1882],\n",
            "         [ 0.1255, -0.0644, -0.1305,  ..., -0.0192,  0.2340,  0.2966]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating helper functions to process the data"
      ],
      "metadata": {
        "id": "1xQiSwmB6km7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data loaded from the en and fr files in the project\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def load_data(file_path):\n",
        "    with open(file_path, encoding='utf-8') as file:\n",
        "        lines = [line.strip().lower() for line in file if line.strip()]\n",
        "    return lines\n",
        "\n",
        "def preprocess(lang_sentences, percentages, berts, bert_tokenizers, DEVICE=\"cuda\", max_len=10):\n",
        "    assert len(berts) == len(bert_tokenizers) == len(percentages), \"The lengths of berts, tokenizer, and percentages must be the same.\"\n",
        "\n",
        "    for i in range(len(berts)):\n",
        "\n",
        "      berts[i].to(DEVICE)\n",
        "\n",
        "    languages_mashed = []\n",
        "    for sentences in zip(*lang_sentences):\n",
        "        sentence_embeddings = []\n",
        "        for lang_index, sentence in enumerate(sentences):\n",
        "            # Tokenize and cut the sentence\n",
        "            tokens = tokenize_and_cut(sentence, bert_tokenizers[lang_index], 10)\n",
        "            input_ids = bert_tokenizers[lang_index].convert_tokens_to_ids(tokens)\n",
        "\n",
        "            input_tensor = torch.tensor([input_ids]).to(DEVICE)\n",
        "\n",
        "            embedding = berts[lang_index](input_tensor)[0].detach()\n",
        "            sentence_embeddings.append(embedding)\n",
        "\n",
        "        combined_embedding = sum([emb * perc for emb, perc in zip(sentence_embeddings, percentages)])\n",
        "        languages_mashed.append(combined_embedding)\n",
        "    return languages_mashed"
      ],
      "metadata": {
        "id": "o5qSMHmpBVvE"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#PreProccessing and creating data"
      ],
      "metadata": {
        "id": "vVDAi9_J6y3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Load English and French data\n",
        "import numpy as np\n",
        "\n",
        "english_sentences = load_data('europarl-v7.fr-en.en')\n",
        "french_sentences = load_data('europarl-v7.fr-en.fr')\n",
        "print(len(english_sentences))\n",
        "print(len(french_sentences))\n",
        "print(english_sentences[1])\n",
        "print(french_sentences[1])\n",
        "# **mish mash** with 0.5 and 0.5 percentage points\n",
        "mashed_sentences = preprocess([english_sentences, french_sentences],\n",
        "                          [0.5, 0.5],\n",
        "                          [bert_en, bert_fr],\n",
        "                          [tokenizer_en, tokenizer_fr],\n",
        "                          \"cuda\")\n",
        "print(mashed_sentences)\n",
        "np.save('mashed_sentences_fr_en_50_50_w_bert_no_rand.npy', mashed_sentences) # saving data so doesnt haev to be loaded again"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 463
        },
        "id": "oKe8T5Je6yPb",
        "outputId": "42bed271-daf8-457f-b668-d58c4f604123"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "263192\n",
            "213802\n",
            "i declare resumed the session of the european parliament adjourned on friday 17 december 1999, and i would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n",
            "je déclare reprise la session du parlement européen qui avait été interrompue le vendredi 17 décembre dernier et je vous renouvelle tous mes vux en espérant que vous avez passé de bonnes vacances.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-125-c8c9db0b955d>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrench_sentences\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# **mish mash** with 0.5 and 0.5 percentage points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m mashed_sentences = preprocess([english_sentences, french_sentences],\n\u001b[0m\u001b[1;32m     12\u001b[0m                           \u001b[0;34m[\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                           \u001b[0;34m[\u001b[0m\u001b[0mbert_en\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_fr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-77-020fac45ad44>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(lang_sentences, percentages, berts, bert_tokenizers, DEVICE, max_len)\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0minput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0membedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mberts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0msentence_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1510\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1511\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1512\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1521\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1522\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You cannot specify both input_ids and inputs_embeds at the same time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    934\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minput_ids\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 935\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn_if_padding_and_no_attention_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    936\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    937\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mwarn_if_padding_and_no_attention_mask\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m   4345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4346\u001b[0m         \u001b[0;31m# Check only the first and last input IDs to reduce overhead.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4347\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4348\u001b[0m             warn_string = (\n\u001b[1;32m   4349\u001b[0m                 \u001b[0;34m\"We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36m__contains__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   1076\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1079\u001b[0m         r\"\"\"Check if `element` is present in tensor\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Here after processing the data it can be loaded with pytorch data loader object"
      ],
      "metadata": {
        "id": "xCCxfgeW8xQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class MashedDataset(Dataset):\n",
        "  def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      return self.data[idx]\n",
        "\n",
        "mashed_sentences = np.load('mashed_sentences_fr_en_50_50_w_bert_no_rand.npy')\n",
        "print(mashed_sentences.shape)\n",
        "\n",
        "#mashed_sentences = mashed_sentences[:]\n",
        "# only using first 100\n",
        "print(mashed_sentences.shape)\n",
        "\n",
        "mashed_sentences_dataset = MashedDataset(torch.from_numpy(mashed_sentences))\n",
        "# make sure to from numpy it\n",
        "\n",
        "batch_size = 32\n",
        "mashed_sentences_data_loader = DataLoader(mashed_sentences_dataset,\n",
        "                                   batch_size=batch_size,\n",
        "                                   shuffle=True,\n",
        "                                    drop_last=True)"
      ],
      "metadata": {
        "id": "wtPus2ll81za",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a2cb18a-a3f8-4a8c-fb3e-75c5b95de3e7"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 1, 10, 768)\n",
            "(2, 1, 10, 768)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Here is the onehot outputs for characters and their encodings"
      ],
      "metadata": {
        "id": "edOXydVaNRl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_characters = ['a', 'à','â', 'æ', 'b', 'c', 'ç', 'd', 'e', 'é', 'è', 'ê',\n",
        "                      'ë', 'œ', 'f', 'g', 'h', 'i', 'î', 'ï', 'j', 'k', 'l',\n",
        "                      'm', 'n', 'o', 'ô', 'p', 'q', 'r', 's', 't', 'u', 'ù','û',\n",
        "                      'ü', 'v', 'w', 'x', 'y', 'ÿ' 'z', \"'\", ' ', ' ', ' ']\n",
        "\n",
        "                      # space character and ' included forcing more spaces\n",
        "# this is both english and french characters discluding the overlap\n",
        "# capitals are **banned** and arent used\n",
        "# helper dictionaries for conversions\n",
        "char_to_index = {char: index for index, char in enumerate(one_hot_characters)}\n",
        "index_to_char = {index: char for index, char in enumerate(one_hot_characters)}\n"
      ],
      "metadata": {
        "id": "56Uuq_xGNXPb"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generator Architecture\n",
        "\n",
        "The way this works is it will take in some length vector and then from it it will create the one hot matrix which represents the generated mish mashed sentence\n"
      ],
      "metadata": {
        "id": "A406yrbCOu05"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_size=100, seq_length=60):\n",
        "        # since only at max 10 words maybe 60 characters can be outputted max\n",
        "        super(Generator, self).__init__()\n",
        "        self.seq_length = seq_length\n",
        "        self.gru = nn.GRU(input_size, 256, num_layers=1, batch_first=True)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear = nn.Linear(256, len(one_hot_characters))\n",
        "        self.softmax = nn.Softmax(dim=2)\n",
        "\n",
        "        # Initialize weights\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.GRU):\n",
        "                nn.init.xavier_uniform_(m.weight_ih_l0)\n",
        "                nn.init.xavier_uniform_(m.weight_hh_l0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "\n",
        "    def forward(self, x, temperature=1.0):\n",
        "    # Repeat the input noise vector seq_length times to create a sequence\n",
        "      x = x.repeat(1, self.seq_length, 1)\n",
        "      out, _ = self.gru(x)  # Only take the output, ignore the hidden state\n",
        "      out = self.relu(out)\n",
        "      out = self.linear(out)\n",
        "      out = out / temperature  # Apply the temperature parameter\n",
        "      out = self.softmax(out)\n",
        "      return out\n"
      ],
      "metadata": {
        "id": "JYJT6zESBoG-"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Call the generator to see what it outputs untrained"
      ],
      "metadata": {
        "id": "DBJvX0LEVxWx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sequences(generator, noise, DEVICE='cuda'):\n",
        "    # Initialize an empty list to store the sequences\n",
        "    sequences = []\n",
        "    # Iterate over the batch dimension of the noise tensor\n",
        "    for i in range(noise.size(0)):\n",
        "        # Generate a sequence of one-hot vectors for each noise vector\n",
        "        one_hot_sequence = generator(noise[i].unsqueeze(0).to(DEVICE), 1)\n",
        "        # Convert the one-hot vectors to character indices\n",
        "        char_indices = torch.argmax(one_hot_sequence, dim=2)\n",
        "        # Convert the character indices to characters\n",
        "        sequence = ''.join(index_to_char[index.item()] for index in char_indices[0])\n",
        "        sequences.append(sequence)\n",
        "    return sequences\n",
        "\n",
        "# this is just for testing not important\n",
        "noise = torch.randn(2, 100)\n",
        "generator = Generator().to('cuda')\n",
        "sequences = generate_sequences(generator, noise)\n",
        "\n",
        "print(sequences)\n",
        "\n",
        "# ill embed this into english and then french and combine it to see what should\n",
        "# happen from it\n",
        "\n",
        "def sequence_to_mash_embed(sequences, percentages, berts, bert_tokenizers, DEVICE=\"cuda\"):\n",
        "    assert len(berts) == len(bert_tokenizers) == len(percentages), \"The lengths of berts, tokenizer, and percentages must be the same.\"\n",
        "\n",
        "    # Initialize an empty list to store the embeddings\n",
        "    mashed_embeddings = []\n",
        "\n",
        "    for i in range(len(berts)): # the sequences are tensors\n",
        "      berts[i].to(DEVICE)\n",
        "\n",
        "    for sequence in sequences:\n",
        "        sentence_embeddings = []\n",
        "        for lang_index in range(len((berts))):\n",
        "            # Tokenize and cut the sentence\n",
        "            tokens = tokenize_and_cut(sequence, bert_tokenizers[lang_index], 10)\n",
        "            input_ids = bert_tokenizers[lang_index].convert_tokens_to_ids(tokens)\n",
        "\n",
        "            input_tensor = torch.tensor([input_ids]).to(DEVICE)\n",
        "\n",
        "            embedding = berts[lang_index](input_tensor)[0].detach()\n",
        "            sentence_embeddings.append(embedding)\n",
        "\n",
        "        combined_embedding = sum([emb * perc for emb, perc in zip(sentence_embeddings, percentages)])\n",
        "        combined_embedding = combined_embedding.squeeze(0)  # Remove the extra dimension\n",
        "        mashed_embeddings.append(combined_embedding)\n",
        "    return mashed_embeddings\n",
        "\n",
        "mashed_embedding = sequence_to_mash_embed(sequences, [0.5, 0.5],\n",
        "                          [bert_en, bert_fr],\n",
        "                          [tokenizer_en, tokenizer_fr],\n",
        "                          \"cuda\")\n",
        "print(mashed_embedding) # this might work\n",
        "# otherwise some other embedding scheme needs to be defined for the mashed language\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrjuraB0V0bF",
        "outputId": "f7d6840a-d251-4170-d7e8-9bb20aff3fd1"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['cèèèèèèddddddddddddddddddddddddddddddddddddddddddddddddddddd', 'mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm']\n",
            "[tensor([[-2.6855e-01, -4.3689e-02,  5.3275e-01,  ...,  2.3556e-02,\n",
            "         -1.4298e-02,  7.4760e-02],\n",
            "        [-2.6275e-01, -2.4350e-01,  5.9643e-01,  ...,  1.6972e-02,\n",
            "         -1.3369e-01, -6.7206e-02],\n",
            "        [-2.5890e-01, -2.3672e-01,  6.2150e-01,  ..., -1.1403e-02,\n",
            "         -1.1334e-01, -1.0422e-01],\n",
            "        ...,\n",
            "        [-2.8967e-01, -6.2265e-02,  5.1663e-01,  ..., -7.2447e-02,\n",
            "         -1.1124e-01, -3.1652e-02],\n",
            "        [-2.8120e-01,  5.8504e-04,  4.5349e-01,  ..., -2.8149e-02,\n",
            "         -7.1692e-02, -3.6999e-03],\n",
            "        [-2.5462e-01,  1.4983e-01,  3.5797e-01,  ..., -1.7987e-02,\n",
            "         -3.8703e-02,  3.3757e-02]]), tensor([[ 1.2189e-01, -9.1069e-02,  7.1602e-02,  ..., -6.9725e-02,\n",
            "          2.1142e-02,  1.4173e-01],\n",
            "        [ 2.3811e-01,  2.1341e-01,  1.7054e-01,  ...,  3.0200e-01,\n",
            "         -1.4037e-01,  5.4780e-02],\n",
            "        [ 2.2187e-01,  2.3537e-01,  1.4329e-01,  ...,  3.1292e-01,\n",
            "         -8.5950e-02, -3.5086e-03],\n",
            "        ...,\n",
            "        [ 2.8347e-01,  2.2118e-01,  1.7147e-01,  ...,  3.4210e-01,\n",
            "         -4.3086e-02, -2.8676e-03],\n",
            "        [ 2.7635e-01,  2.2543e-01,  1.6657e-01,  ...,  3.3642e-01,\n",
            "         -3.7522e-02,  1.2732e-04],\n",
            "        [ 2.7096e-01,  2.5430e-01,  1.7497e-01,  ...,  3.3200e-01,\n",
            "         -3.6875e-02, -6.6658e-03]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Discriminator Architecture\n",
        "\n",
        "This has to figure out if something is real or fake"
      ],
      "metadata": {
        "id": "aWaTjcFcOwzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_size=768, hidden_size=128):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.gru = nn.GRU(input_size, hidden_size, num_layers=1, batch_first=True)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.linear = nn.Linear(hidden_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru(x)  # Only take the output, ignore the hidden state\n",
        "        out = self.relu(out)\n",
        "        out = self.linear(out[:, -1, :])  # Only take the last output of the sequence\n",
        "        out = self.sigmoid(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "_KVJQO-hOzQu"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Testing the untrained discriminator on the previous embeddings"
      ],
      "metadata": {
        "id": "_MnSlVwlbPhx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the numpy array to a PyTorch tensor\n",
        "\n",
        "mashed_embedding_tensor = torch.stack(mashed_embedding)\n",
        "\n",
        "# Pass the tensor to the discriminator\n",
        "discriminator = Discriminator().to('cuda')\n",
        "output = discriminator(mashed_embedding_tensor)\n",
        "\n",
        "# Pass the embeddings through the discriminator\n",
        "print(output)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDyeqQ9-bUdR",
        "outputId": "242d6184-33ba-4eb1-e70a-04da94af4b33"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5336],\n",
            "        [0.5700]], grad_fn=<SigmoidBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Setup"
      ],
      "metadata": {
        "id": "wSQUUjXCO5en"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(DEVICE)\n",
        "D = Discriminator().to(DEVICE)\n",
        "G = Generator(seq_length=32).to(DEVICE)\n",
        "# shoving hopefully everything to the gpu\n",
        "\n",
        "max_epoch = 50 # going for 50 epochs\n",
        "step = 0 # step through the data\n",
        "n_noise = 100 # size of noise vector\n",
        "\n",
        "criterion = nn.BCELoss()\n",
        "D_opt = torch.optim.Adam(D.parameters(), lr=10e-10, betas=(0.5, 0.999))\n",
        "G_opt = torch.optim.Adam(G.parameters(), lr=10e-10, betas=(0.5, 0.999))\n",
        "# optimizers for both the discriminator and generator alongside a\n",
        "# binary cross entropy loss\n",
        "\n",
        "# We will denote real images as 1s and fake images as 0s\n",
        "# This is why we needed to drop the last batch of the data loader\n",
        "D_labels = torch.ones([batch_size, 1]).to(DEVICE) # Discriminator label: real\n",
        "D_fakes = torch.zeros([batch_size, 1]).to(DEVICE) # Discriminator Label: fake"
      ],
      "metadata": {
        "id": "Wg-pAhhbO7du",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3ecf553-b0eb-4555-e8ca-aa1b4b1087a4"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Loop"
      ],
      "metadata": {
        "id": "xRY_j8alO1lJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "# import pyplot to plot images\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "start_time = time.time()\n",
        "for epoch in range(max_epoch):\n",
        "    for idx, word_embeddings in enumerate(mashed_sentences_data_loader):\n",
        "        # Training Discriminator\n",
        "        x = word_embeddings.float().to(DEVICE)\n",
        "        # rehsaping x to have the x dimension in it for the vectores\n",
        "        x = x.view(batch_size, 1, 768)\n",
        "        x_outputs = D(x) # input includes labels\n",
        "        D_x_loss = criterion(x_outputs, D_labels) # Discriminator loss for real images\n",
        "\n",
        "        z = torch.randn(batch_size, n_noise).to(DEVICE)\n",
        "\n",
        "        # the generator outputs a sequence that sequence must then be converted into\n",
        "        # embeddings that are passsed to the dsicriminator\n",
        "         # the generator and the randomness to make a sequence\n",
        "        sequences = generate_sequence(G, z)\n",
        "        z_outputs = D(sequence_to_mash_embed(sequences, DEVICE)) # input to both generator and discriminator includes labels\n",
        "        D_z_loss = criterion(z_outputs, D_fakes) # Discriminator loss for fake images\n",
        "        D_loss = D_x_loss + D_z_loss # Total Discriminator loss\n",
        "\n",
        "        D.zero_grad()\n",
        "        D_loss.backward()\n",
        "        D_opt.step()\n",
        "        # updating the discriminator model\n",
        "\n",
        "        # Training Generator\n",
        "        z = torch.randn(batch_size, n_noise).to(DEVICE) # creating the random vector alongside the batch proper\n",
        "        train_sequences = generate_sequence(G, z)\n",
        "        z_outputs = D(sequence_to_mash_embed(train_sequences, DEVICE))\n",
        "        G_loss = -1 * criterion(z_outputs, D_fakes) # Generator loss is negative disciminator loss\n",
        "\n",
        "        G.zero_grad()\n",
        "        G_loss.backward()\n",
        "        G_opt.step()\n",
        "        # updating the generator model\n",
        "\n",
        "        if step % 500 == 0:\n",
        "            print('Epoch: {}/{}, Step: {}, D Loss: {}, G Loss: {} time: '.format(epoch, max_epoch, step, D_loss.item(), G_loss.item(), time.time() - start_time))\n",
        "            # done to view teh loss\n",
        "        step += 1\n",
        "\n",
        "    if epoch+1 in [1, 5, 10, 15, 20, 25, 30, 50]:\n",
        "      # if in the 1st (done for making sure everything is good)\n",
        "      # or the 10th or 30th or 50th epoch then display what the\n",
        "      # generator has so far\n",
        "      model_save_name = 'discriminator.pt'\n",
        "      path = F\"/content/gdrive/My Drive/{model_save_name}\"\n",
        "      torch.save(D.state_dict(), path)\n",
        "\n",
        "      model_save_name = 'generator.pt'\n",
        "      path = F\"/content/gdrive/My Drive/{model_save_name}\"\n",
        "      torch.save(G.state_dict(), path)\n",
        "\n",
        "      print(f\"on epoch {epoch + 1}\")\n",
        "      noise = torch.randn(1, 100).to(DEVICE)\n",
        "      G.eval()  # eval mode\n",
        "      sequences = generate_sequence(generator, noise)\n",
        "      print(sequences)\n",
        "      # show the plot from get sample images\n",
        "      G.train()\n",
        "      # back to trianing the genrator\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IADUvzIQBrAS",
        "outputId": "da6b11dd-8768-41d1-d864-14b59ee2544d"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "on epoch 1\n",
            "['llllll                                                      ']\n",
            "on epoch 5\n",
            "['rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr']\n",
            "on epoch 10\n",
            "['eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee']\n",
            "on epoch 15\n",
            "['mmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmmm']\n",
            "on epoch 20\n",
            "[\"''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\"]\n",
            "on epoch 25\n",
            "['sssssssssssaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa']\n",
            "on epoch 30\n",
            "['ææææææææææææææææææææææææææææææææææææææææææææææææææææææææææææ']\n",
            "on epoch 50\n",
            "['cccccccccccccccccccccccccccccccccccccccccccccccccccccccccccc']\n"
          ]
        }
      ]
    }
  ]
}